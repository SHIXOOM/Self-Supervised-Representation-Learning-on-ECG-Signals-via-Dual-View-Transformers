import os
from dataclasses import asdict
from pathlib import Path
import torch
from src import DualAugmenter


class CheckpointManager:
    """
    Manages model checkpointing during training.
    
    Features:
    - Saves model, optimizer, and training state after each epoch
    - Keeps track of best model based on lowest loss
    - Maintains only the last N checkpoints to save disk space
    - Supports resuming training from saved checkpoints
    """
    
    def __init__(
        self,
        checkpoint_dir: str = "../models/checkpoints",
        keep_last_n: int = 3,
        save_best: bool = True
    ):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.keep_last_n = keep_last_n
        self.save_best = save_best
        self.best_loss = float('inf')
        
    def save_checkpoint(
        self,
        model,
        optimizer,
        epoch: int,
        loss: float,
        config,
        additional_info: dict = None
    ):
        """Save checkpoint with model state, optimizer state, and metadata."""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'config': asdict(config),
        }
        
        if additional_info:
            checkpoint.update(additional_info)
        
        # Save regular epoch checkpoint
        checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}.pt"
        torch.save(checkpoint, checkpoint_path)
        print(f"Saved checkpoint: {checkpoint_path}")
        
        # Save best model if this is the lowest loss
        if self.save_best and loss < self.best_loss:
            self.best_loss = loss
            best_path = self.checkpoint_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
            print(f"New best model saved with loss: {loss:.4f}")
        
        # Save latest checkpoint (for easy resume)
        latest_path = self.checkpoint_dir / "latest.pt"
        torch.save(checkpoint, latest_path)
        
        # Clean up old checkpoints
        self._cleanup_old_checkpoints()
    
    def _cleanup_old_checkpoints(self):
        """Keep only the last N checkpoints to save disk space."""
        checkpoints = sorted(
            self.checkpoint_dir.glob("checkpoint_epoch_*.pt"),
            key=lambda p: int(p.stem.split('_')[-1])
        )
        
        # Remove old checkpoints beyond keep_last_n
        if len(checkpoints) > self.keep_last_n:
            for old_checkpoint in checkpoints[:-self.keep_last_n]:
                old_checkpoint.unlink()
                print(f"Removed old checkpoint: {old_checkpoint.name}")
    
    def load_checkpoint(self, checkpoint_path: str, model, optimizer=None, map_location=None):
        """
        Load checkpoint and restore model and optimizer states.
        
        Returns:
            Dictionary with epoch, loss, and config information
        """
        checkpoint = torch.load(checkpoint_path, map_location=map_location)
        model.load_state_dict(checkpoint['model_state_dict'])
        
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        print(f"Loaded checkpoint from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}")
        return checkpoint
    
    def get_latest_checkpoint(self):
        """Get path to the latest checkpoint if it exists."""
        latest_path = self.checkpoint_dir / "latest.pt"
        return str(latest_path) if latest_path.exists() else None